In this assignment, I implemented key components of a Recurrent Neural Network in numpy, including:

## 1 - Forward propagation for the basic Recurrent Neural Network

## 2-   Fprward Pass for Long Short-Term Memory (LSTM) network

## 3-

Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have "memory". 

They can read inputs x⟨t⟩ (such as words) one at a time, and remember some information/context through the hidden layer activations
that get passed from one time-step to the next. 
This allows a unidirectional RNN to take information from the past to process later inputs.

A bidirectional RNN can take context from both the past and the future. 
